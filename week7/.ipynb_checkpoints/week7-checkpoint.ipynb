{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features names with NaN\n",
      "\n",
      "first_blood_time\n",
      "first_blood_team\n",
      "first_blood_player1\n",
      "first_blood_player2\n",
      "radiant_bottle_time\n",
      "radiant_courier_time\n",
      "radiant_flying_courier_time\n",
      "radiant_first_ward_time\n",
      "dire_bottle_time\n",
      "dire_courier_time\n",
      "dire_flying_courier_time\n",
      "dire_first_ward_time\n",
      "\n",
      "\n",
      "GRADIENT BOOSTING\n",
      "\n",
      "\n",
      "Building gradient boosting classifier\n",
      "\n",
      "\n",
      "Number of Trees: 10 \n",
      "Score: 0.6645 \n",
      "Time to build classifier: 28.08 sec\n",
      "\n",
      "Number of Trees: 20 \n",
      "Score: 0.682 \n",
      "Time to build classifier: 53.58 sec\n",
      "\n",
      "Number of Trees: 30 \n",
      "Score: 0.6895 \n",
      "Time to build classifier: 77.72 sec\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "\n",
      "\n",
      "Building classifier w/ dropping categories info\n",
      "\n",
      "Best C: 0.67, \n",
      "best score 0.71644, \n",
      "best time 11.116078\n",
      "\n",
      "\n",
      "Building classifier with dropping categories info\n",
      "\n",
      "Best C: 0.78, \n",
      "best score 0.71642, \n",
      "best time 9.837326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmakhotin/opt/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:33: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/dmakhotin/opt/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building classifier with one hot encoding \n",
      "\n",
      "Best C: 0.11, \n",
      "best score 0.75184, \n",
      "best time 20.468194\n",
      "\n",
      "\n",
      "\n",
      "Running best classifier on test data\n",
      "\n",
      "\n",
      "Logistic regression with one hot encoding  on test data\n",
      "\n",
      "Predict min 0.003377390470058028\n",
      "Predict max 0.996622609529942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\xd0\\x9f\\xd0\\xbe\\xd0\\xb4\\xd1\\x85\\xd0\\xbe\\xd0\\xb4 2: \\xd0\\xbb\\xd0\\xbe\\xd0\\xb3\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x8f \\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd1\\x80\\xd0\\xb5\\xd1\\x81\\xd1\\x81\\xd0\\xb8\\xd1\\x8f\\n\\n1. \\xd0\\x9a\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb5 \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x83\\xd1\\x87\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd1\\x81\\xd1\\x8c \\xd1\\x83 \\xd0\\xbb\\xd0\\xbe\\xd0\\xb3\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb9 \\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd1\\x80\\xd0\\xb5\\xd1\\x81\\xd1\\x81\\xd0\\xb8\\xd0\\xb8 \\xd0\\xbd\\xd0\\xb0\\xd0\\xb4 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5\\xd0\\xbc\\xd0\\xb8 \\xd0\\xb8\\xd1\\x81\\xd1\\x85\\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd1\\x8b\\xd0\\xbc\\xd0\\xb8 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xb0\\xd0\\xbc\\xd0\\xb8?\\n\\xd0\\x9a\\xd0\\xb0\\xd0\\xba \\xd0\\xbe\\xd0\\xbd\\xd0\\xbe \\xd1\\x81\\xd0\\xbe\\xd0\\xbe\\xd1\\x82\\xd0\\xbd\\xd0\\xbe\\xd1\\x81\\xd0\\xb8\\xd1\\x82\\xd1\\x81\\xd1\\x8f \\xd1\\x81 \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe\\xd0\\xbc \\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xb4\\xd0\\xb8\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd0\\xb1\\xd1\\x83\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd0\\xbd\\xd0\\xb3\\xd0\\xb0? \\xd0\\xa7\\xd0\\xb5\\xd0\\xbc \\xd0\\xb2\\xd1\\x8b \\xd0\\xbc\\xd0\\xbe\\xd0\\xb6\\xd0\\xb5\\xd1\\x82\\xd0\\xb5 \\xd0\\xbe\\xd0\\xb1\\xd1\\x8a\\xd1\\x8f\\xd1\\x81\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd1\\x8c \\xd1\\x8d\\xd1\\x82\\xd1\\x83 \\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd0\\xbd\\xd0\\xb8\\xd1\\x86\\xd1\\x83?\\n\\xd0\\x91\\xd1\\x8b\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xb5\\xd0\\xb5 \\xd0\\xbb\\xd0\\xb8 \\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd0\\xb0\\xd0\\xb5\\xd1\\x82 \\xd0\\xbb\\xd0\\xbe\\xd0\\xb3\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x8f \\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd1\\x80\\xd0\\xb5\\xd1\\x81\\xd1\\x81\\xd0\\xb8\\xd1\\x8f \\xd0\\xbf\\xd0\\xbe \\xd1\\x81\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbd\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd1\\x8e \\xd1\\x81 \\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xb4\\xd0\\xb8\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd0\\xbd\\xd1\\x8b\\xd0\\xbc \\xd0\\xb1\\xd1\\x83\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd0\\xbd\\xd0\\xb3\\xd0\\xbe\\xd0\\xbc?\\n\\n\\xd0\\x9e\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82:\\n\\n\\xd0\\x9a\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe - 0.7163.\\n\\xd0\\x92\\xd1\\x8b\\xd1\\x88\\xd0\\xb5 \\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xb4\\xd0\\xb8\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd0\\xb1\\xd1\\x83\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd0\\xbd\\xd0\\xb3\\xd0\\xb0 \\xd0\\xbd\\xd0\\xb0 0.035. \\xd0\\xa0\\xd0\\xb0\\xd0\\xb7\\xd0\\xbd\\xd0\\xb8\\xd1\\x86\\xd1\\x83 \\xd0\\xbc\\xd0\\xbe\\xd0\\xb6\\xd0\\xbd\\xd0\\xbe \\xd0\\xbe\\xd0\\xb1\\xd1\\x8a\\xd1\\x8f\\xd1\\x81\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd1\\x8c \\xd0\\xbc\\xd0\\xb0\\xd1\\x81\\xd1\\x88\\xd1\\x82\\xd0\\xb0\\xd0\\xb1\\xd0\\xb8\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5\\xd0\\xbc \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\n\\xd0\\x97\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb8\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe \\xd0\\xb1\\xd1\\x8b\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xb5\\xd0\\xb5.\\n\\n\\n2.\\xd0\\x9a\\xd0\\xb0\\xd0\\xba \\xd0\\xb2\\xd0\\xbb\\xd0\\xb8\\xd1\\x8f\\xd0\\xb5\\xd1\\x82 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd0\\xbb\\xd0\\xbe\\xd0\\xb3\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb9 \\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd1\\x80\\xd0\\xb5\\xd1\\x81\\xd1\\x81\\xd0\\xb8\\xd0\\xb8 \\xd1\\x83\\xd0\\xb4\\xd0\\xb0\\xd0\\xbb\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xb5\\xd0\\xb3\\xd0\\xbe\\xd1\\x80\\xd0\\xb8\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x8b\\xd1\\x85 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb2 (\\xd1\\x83\\xd0\\xba\\xd0\\xb0\\xd0\\xb6\\xd0\\xb8\\xd1\\x82\\xd0\\xb5 \\xd0\\xbd\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb5 \\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd1\\x80\\xd0\\xb8\\xd0\\xba\\xd0\\xb8 \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xb0)?\\n\\xd0\\xa7\\xd0\\xb5\\xd0\\xbc \\xd0\\xb2\\xd1\\x8b \\xd0\\xbc\\xd0\\xbe\\xd0\\xb6\\xd0\\xb5\\xd1\\x82\\xd0\\xb5 \\xd0\\xbe\\xd0\\xb1\\xd1\\x8a\\xd1\\x8f\\xd1\\x81\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd1\\x8c \\xd1\\x8d\\xd1\\x82\\xd0\\xbe \\xd0\\xb8\\xd0\\xb7\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5?\\n\\n\\xd0\\x9e\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82:\\n\\n\\xd0\\x9d\\xd0\\xb5 \\xd0\\xb2\\xd0\\xbb\\xd0\\xb8\\xd1\\x8f\\xd0\\xb5\\xd1\\x82. \\xd0\\x9f\\xd0\\xbe\\xd0\\xba\\xd0\\xb0 \\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xb5\\xd0\\xb3\\xd0\\xbe\\xd1\\x80\\xd0\\xb8\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x8b\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xb8 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb5\\xd0\\xbe\\xd0\\xb1\\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd1\\x8b \\xd0\\xba \\xd0\\xbc\\xd0\\xb5\\xd1\\x88\\xd0\\xba\\xd1\\x83 \\xd1\\x81\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2, \\xd0\\xbe\\xd0\\xbd\\xd0\\xb8, \\xd0\\xbe\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd0\\xb4\\xd0\\xbd\\xd0\\xbe, \\xd0\\xbd\\xd0\\xb5 \\xd1\\x83\\xd1\\x87\\xd0\\xb0\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd1\\x83\\xd1\\x8e\\xd1\\x82 \\xd0\\xb2 \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xbe\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb8\\n\\xd0\\xba\\xd0\\xbb\\xd0\\xb0\\xd1\\x81\\xd1\\x81\\xd0\\xb8\\xd1\\x84\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xb0.\\n\\n\\xd0\\x9d\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb5 \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe - 0.7163\\n\\n\\n3. \\xd0\\xa1\\xd0\\xba\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xba\\xd0\\xbe \\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd0\\xbb\\xd0\\xb8\\xd1\\x87\\xd0\\xbd\\xd1\\x8b\\xd1\\x85 \\xd0\\xb8\\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd0\\xb8\\xd1\\x84\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb2 \\xd0\\xb3\\xd0\\xb5\\xd1\\x80\\xd0\\xbe\\xd0\\xb5\\xd0\\xb2 \\xd1\\x81\\xd1\\x83\\xd1\\x89\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd1\\x83\\xd0\\xb5\\xd1\\x82 \\xd0\\xb2 \\xd0\\xb4\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe\\xd0\\xb9 \\xd0\\xb8\\xd0\\xb3\\xd1\\x80\\xd0\\xb5?\\n\\n\\xd0\\x9e\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82:\\n\\n108 \\xd0\\xb8\\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd0\\xb8\\xd1\\x84\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb2. \\xd0\\x97\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb0\\xd0\\xba\\xd1\\x81\\xd0\\xb8\\xd0\\xbc\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe - 112. \\xd0\\x9f\\xd0\\xbe \\xd0\\xba\\xd0\\xb0\\xd0\\xba\\xd0\\xb8\\xd0\\xbc-\\xd1\\x82\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd1\\x87\\xd0\\xb8\\xd0\\xbd\\xd0\\xb0\\xd0\\xbc \\xd0\\xbd\\xd0\\xb5\\xd0\\xba\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd1\\x8b\\xd0\\xb5 \\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xbf\\xd1\\x83\\xd1\\x89\\xd0\\xb5\\xd0\\xbd\\xd1\\x8b.\\n\\n\\n4. \\xd0\\x9a\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x83\\xd1\\x87\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd1\\x81\\xd1\\x8c \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xb8 \\xd0\\xb4\\xd0\\xbe\\xd0\\xb1\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb8 \"\\xd0\\xbc\\xd0\\xb5\\xd1\\x88\\xd0\\xba\\xd0\\xb0 \\xd1\\x81\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2\" \\xd0\\xbf\\xd0\\xbe \\xd0\\xb3\\xd0\\xb5\\xd1\\x80\\xd0\\xbe\\xd1\\x8f\\xd0\\xbc?\\n\\xd0\\xa3\\xd0\\xbb\\xd1\\x83\\xd1\\x87\\xd1\\x88\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd1\\x81\\xd1\\x8c \\xd0\\xbb\\xd0\\xb8 \\xd0\\xbe\\xd0\\xbd\\xd0\\xbe \\xd0\\xbf\\xd0\\xbe \\xd1\\x81\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbd\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd1\\x8e \\xd1\\x81 \\xd0\\xbf\\xd1\\x80\\xd0\\xb5\\xd0\\xb4\\xd1\\x8b\\xd0\\xb4\\xd1\\x83\\xd1\\x89\\xd0\\xb8\\xd0\\xbc \\xd0\\xb2\\xd0\\xb0\\xd1\\x80\\xd0\\xb8\\xd0\\xb0\\xd0\\xbd\\xd1\\x82\\xd0\\xbe\\xd0\\xbc? \\xd0\\xa7\\xd0\\xb5\\xd0\\xbc \\xd0\\xb2\\xd1\\x8b \\xd0\\xbc\\xd0\\xbe\\xd0\\xb6\\xd0\\xb5\\xd1\\x82\\xd0\\xb5 \\xd1\\x8d\\xd1\\x82\\xd0\\xbe \\xd0\\xbe\\xd0\\xb1\\xd1\\x8a\\xd1\\x8f\\xd1\\x81\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd1\\x8c?\\n\\n\\xd0\\x9e\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82:\\n\\n\\xd0\\x9d\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb5 \\xd0\\xba\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe - 0.7516\\n\\xd0\\x9a\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd0\\xb2\\xd1\\x8b\\xd1\\x80\\xd0\\xbe\\xd1\\x81\\xd0\\xbb\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0 0.0353\\n\\xd0\\xa2\\xd0\\xb5\\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd1\\x8c \\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xb5\\xd0\\xb3\\xd0\\xbe\\xd1\\x80\\xd0\\xb8\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x8b\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xba\\xd0\\xb8 \\xd1\\x83\\xd1\\x87\\xd0\\xb0\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbb\\xd0\\xb8 \\xd0\\xb2 \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xbe\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb8 \\xd0\\xba\\xd0\\xbb\\xd0\\xb0\\xd1\\x81\\xd1\\x81\\xd0\\xb8\\xd1\\x84\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xb0\\n\\n\\n5. \\xd0\\x9a\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb8\\xd0\\xbd\\xd0\\xb8\\xd0\\xbc\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe\\xd0\\xb5 \\xd0\\xb8 \\xd0\\xbc\\xd0\\xb0\\xd0\\xba\\xd1\\x81\\xd0\\xb8\\xd0\\xbc\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe\\xd0\\xb5 \\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb3\\xd0\\xbd\\xd0\\xbe\\xd0\\xb7\\xd0\\xb0 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x82\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb9 \\xd0\\xb2\\xd1\\x8b\\xd0\\xb1\\xd0\\xbe\\xd1\\x80\\xd0\\xba\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x83\\xd1\\x87\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd1\\x81\\xd1\\x8c \\xd1\\x83 \\xd0\\xbb\\xd1\\x83\\xd1\\x87\\xd1\\x88\\xd0\\xb5\\xd0\\xb3\\xd0\\xbe \\xd0\\xb8\\xd0\\xb7 \\xd0\\xb0\\xd0\\xbb\\xd0\\xb3\\xd0\\xbe\\xd1\\x80\\xd0\\xb8\\xd1\\x82\\xd0\\xbc\\xd0\\xbe\\xd0\\xb2?\\n\\n\\xd0\\x9e\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82:\\n\\nMin -  0.00337739165456\\nMax -  0.996622608345\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Predict Dota 2 match results based on first 5 mins match data.\n",
    "Using 2 approaches with various data improvements to compare.\n",
    "Task description - in final-statement.html.\n",
    "There were some questions in the task, and answers should have been\n",
    "provided in comments. Answers are in Russian as solution\n",
    "was reviewed by Russian speaking folks.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'nyash myash'\n",
    "\n",
    "x\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# GRADIENT BOOSTING\n",
    "\n",
    "\n",
    "def bag_of_words(hero_ids, features):\n",
    "    \"\"\"one hot encoding\"\"\"\n",
    "\n",
    "    x_pick = np.zeros((features.shape[0], max(hero_ids)))\n",
    "\n",
    "    for i, match_id in enumerate(features.index):\n",
    "        for p in xrange(5):\n",
    "            x_pick[i, features.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "            x_pick[i, features.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "\n",
    "    return x_pick\n",
    "\n",
    "\n",
    "def predict(clf, x, y):\n",
    "    \"\"\"Scoring function for cross-validation\"\"\"\n",
    "\n",
    "    proba = clf.predict_proba(x)[:, 1]\n",
    "    return roc_auc_score(y, proba)\n",
    "\n",
    "def log_reg(x, y, kf):\n",
    "\n",
    "    c_grid = np.linspace(0.001, 1.0, num=10)\n",
    "    best_score = 0\n",
    "    best_c = 0\n",
    "    corresponding_duration = 0\n",
    "\n",
    "    for c in c_grid:\n",
    "        start_time = datetime.datetime.now()\n",
    "        clf = LogisticRegression(penalty='l2', C=c)\n",
    "        cvs = cross_val_score(clf, x, y, scoring=predict, cv=kf)\n",
    "        score = np.round(np.mean(cvs), 5)\n",
    "        duration = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_c = np.round(c,2)\n",
    "            corresponding_duration = duration\n",
    "\n",
    "    return [best_score, best_c, corresponding_duration]\n",
    "\n",
    "features = pandas.read_csv('features.csv', index_col='match_id')\n",
    "\n",
    "# Get match results and create results vector\n",
    "y = features['radiant_win'].values\n",
    "\n",
    "# Drop columns connected with match results\n",
    "res_data = [u'duration', u'radiant_win', u'tower_status_radiant', u'tower_status_dire', u'barracks_status_radiant',\n",
    "            u'barracks_status_dire']\n",
    "features.drop(res_data, axis=1, inplace=True)\n",
    "\n",
    "features.drop('start_time', axis=1, inplace=True) # dropping to avoid warnings when using logistic regression\n",
    "\n",
    "# Get names of columns with NaN values\n",
    "f = features.count()\n",
    "na_features = f[f != features.shape[0]].index.values\n",
    "print \"Features names with NaN\\n\"\n",
    "for item in na_features:\n",
    "    print item\n",
    "\n",
    "# Fill in NaN\n",
    "features.fillna(value=0, inplace=True)\n",
    "\n",
    "# Create matrix of features\n",
    "x = features.values\n",
    "\n",
    "print \"\\n\\nGRADIENT BOOSTING\\n\\n\"\n",
    "print \"Building gradient boosting classifier\\n\"\n",
    "\n",
    "# Build classifier and make cross-validation with AUC-ROC score\n",
    "kf = KFold(n_splits=5, shuffle=True)  # Cross-validation iterator\n",
    "n_trees = [10, 20, 30]\n",
    "for i in n_trees:\n",
    "    start_time = datetime.datetime.now()\n",
    "    clf = GradientBoostingClassifier(n_estimators=i)\n",
    "    cvs = cross_val_score(clf, x, y, scoring=predict, cv=kf)\n",
    "    score = np.round(np.mean(cvs), 4)\n",
    "    duration = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    print \"\\nNumber of Trees: {0} \\nScore: {1} \\nTime to build classifier: {2:.2f} sec\".format(i, score, duration)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Подход 1: градиентный бустинг \"в лоб\"\n",
    "\n",
    "1. Какие признаки имеют пропуски среди своих значений?\n",
    "Что могут означать пропуски в этих признаках (ответьте на этот вопрос для двух любых признаков)?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Признаки, имеющие пропуски:\n",
    "- first_blood_time\n",
    "- first_blood_team\n",
    "- first_blood_player1\n",
    "- first_blood_player2\n",
    "- radiant_bottle_time\n",
    "- radiant_courier_time\n",
    "- radiant_flying_courier_time\n",
    "- radiant_first_ward_time\n",
    "- dire_bottle_time\n",
    "- dire_courier_time\n",
    "- dire_flying_courier_time\n",
    "- dire_first_ward_time\n",
    "\n",
    "Что могут означать пропуски:\n",
    "first_blood_time - за первые 5 мин игры событие не произошло\n",
    "radiant_bottle_time - за время игры ни одна команда не приобрела предмет \"bottle\"\n",
    "\n",
    "2. Как называется столбец, содержащий целевую переменную?\n",
    "\n",
    "Ответ: radiant_win\n",
    "\n",
    "3. Как долго проводилась кросс-валидация для градиентного бустинга с 30 деревьями? Какое качество при этом получилось?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Время кросс-валидации: 269 секунд\n",
    "Качество: 0.69\n",
    "\n",
    "\n",
    "4. Имеет ли смысл использовать больше 30 деревьев в градиентном бустинге?\n",
    "Что бы вы предложили делать, чтобы ускорить его обучение при увеличении количества деревьев?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Использовать больше 30 деревьев смысла нет: качество по сравннию с 10 деревьями выросло незначительно (0.67 для 10\n",
    "деревьев и 0.69 для 30), а время обучения возрасло более чем в 3 раза (88 сек для 10 деревьев и 269 сек для 30)\n",
    "\n",
    "Для увеличения скорости при увеличении количества деревьев можно:\n",
    "- использовать понижение размерности пространста признаков\n",
    "- производить обучение только на части объектов\n",
    "- уменьшить глубину деревьев до 2 (по умолчанию глубина - 3)\n",
    "- использовать более производительную машину)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# 1. Building classifier w/ dropping castegories info\n",
    "\n",
    "print \"\\n\\nLOGISTIC REGRESSION\\n\\n\"\n",
    "print \"Building classifier w/ dropping categories info\\n\"\n",
    "\n",
    "x_1_scaled = scale(x)\n",
    "best_score, best_c, corresponding_duration = log_reg(x_1_scaled, y, kf)\n",
    "print \"Best C: {}, \\nbest score {}, \\nbest time {}\\n\".format(best_c, best_score, corresponding_duration)\n",
    "\n",
    "# 2. Building classifier with dropping categories info\n",
    "\n",
    "heroes = ['r1_hero', 'r2_hero', 'r3_hero', 'r4_hero', 'r5_hero', 'd1_hero', 'd2_hero', 'd3_hero',\n",
    "                  'd4_hero', 'd5_hero']\n",
    "\n",
    "categ_features =['r1_hero', 'r2_hero', 'r3_hero', 'r4_hero', 'r5_hero', 'd1_hero', 'd2_hero', 'd3_hero',\n",
    "                  'd4_hero', 'd5_hero', 'lobby_type']\n",
    "\n",
    "print \"\\nBuilding classifier with dropping categories info\\n\"\n",
    "dropped_categ_features = features.drop(categ_features, axis=1)\n",
    "x_2 = dropped_categ_features.values\n",
    "x_2_scaled = scale(x_2)\n",
    "best_score, best_c, corresponding_duration = log_reg(x_2_scaled, y, kf)\n",
    "print \"Best C: {}, \\nbest score {}, \\nbest time {}\\n\".format(best_c, best_score, corresponding_duration)\n",
    "\n",
    "\n",
    "# 3. Building classifier with  with one hot encoding\n",
    "\n",
    "\n",
    "# Getting heroes id\n",
    "\n",
    "hero_ids = set()\n",
    "for hero in heroes:\n",
    "    hero_ids.update(features[hero].unique())\n",
    "\n",
    "n = len(hero_ids)\n",
    "hero_ids = list(hero_ids)\n",
    "\n",
    "x_pick = bag_of_words(hero_ids, features)\n",
    "x_3_scaled = np.hstack((x_2_scaled, x_pick))\n",
    "\n",
    "\n",
    "print \"\\nBuilding classifier with one hot encoding \\n\"\n",
    "best_score, best_c, corresponding_duration = log_reg(x_3_scaled, y, kf)\n",
    "print \"Best C: {}, \\nbest score {}, \\nbest time {}\\n\".format(best_c, best_score, corresponding_duration)\n",
    "\n",
    "\n",
    "# Running best classifier on test data\n",
    "# Logistic regression with one hot encoding of words showed best AUC-ROC Score\n",
    "\n",
    "print \"\\n\\nRunning best classifier on test data\\n\\n\"\n",
    "print\"Logistic regression with one hot encoding  on test data\\n\"\n",
    "\n",
    "# Fit classifier\n",
    "\n",
    "final_clf = LogisticRegression(penalty='l2', C=0.11)\n",
    "final_clf.fit(x_3_scaled, y)\n",
    "\n",
    "# Preprocess test data\n",
    "\n",
    "features_test = pandas.read_csv('features_test.csv', index_col='match_id')\n",
    "features_test.drop('start_time', axis=1, inplace=True)\n",
    "features_test.fillna(value=0, inplace=True)\n",
    "dropped_categ_features_test = features_test.drop(categ_features, axis=1)\n",
    "x_test_values = dropped_categ_features_test.values\n",
    "x_test_scaled = scale(x_test_values)\n",
    "x_pick_test = bag_of_words(hero_ids, features_test)\n",
    "x_test = np.hstack((x_test_scaled, x_pick_test))\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "res = final_clf.predict_proba(x_test)\n",
    "\n",
    "print \"Predict min\", np.amin(res)\n",
    "print \"Predict max\", np.amax(res)\n",
    "\n",
    "with open('predictions_proba.csv', 'w') as f:\n",
    "    f.write('match_id,radiant_win\\n')\n",
    "    for i, match_id in enumerate(features_test.index):\n",
    "        f.write('{},{}\\n'.format(match_id,res[i][0]))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Подход 2: логистическая регрессия\n",
    "\n",
    "1. Какое качество получилось у логистической регрессии над всеми исходными признаками?\n",
    "Как оно соотносится с качеством градиентного бустинга? Чем вы можете объяснить эту разницу?\n",
    "Быстрее ли работает логистическая регрессия по сравнению с градиентным бустингом?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Качество - 0.7163.\n",
    "Выше градиентного бустинга на 0.035. Разницу можно объяснить масштабированием признаков\n",
    "Значительно быстрее.\n",
    "\n",
    "\n",
    "2.Как влияет на качество логистической регрессии удаление категориальных признаков (укажите новое значение метрики качества)?\n",
    "Чем вы можете объяснить это изменение?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Не влияет. Пока категориальные признаки не преобразованы к мешку слов, они, очевидно, не участвуют в построении\n",
    "классификатора.\n",
    "\n",
    "Новое качество - 0.7163\n",
    "\n",
    "\n",
    "3. Сколько различных идентификаторов героев существует в данной игре?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "108 идентификаторов. Значение максимального - 112. По каким-то причинам некоторые значения пропущены.\n",
    "\n",
    "\n",
    "4. Какое получилось качество при добавлении \"мешка слов\" по героям?\n",
    "Улучшилось ли оно по сравнению с предыдущим вариантом? Чем вы можете это объяснить?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Новое качество - 0.7516\n",
    "Качество выросло на 0.0353\n",
    "Теперь категориальные признаки участвовали в построении классификатора\n",
    "\n",
    "\n",
    "5. Какое минимальное и максимальное значение прогноза на тестовой выборке получилось у лучшего из алгоритмов?\n",
    "\n",
    "Ответ:\n",
    "\n",
    "Min -  0.00337739165456\n",
    "Max -  0.996622608345\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
